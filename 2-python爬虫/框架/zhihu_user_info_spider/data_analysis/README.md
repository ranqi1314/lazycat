### 数据分析模块【该模块必须运行在爬虫模块之后】

该模块的主要作用是对爬虫模块中爬取的数据进行清洗，以及数据分析。

#### 数据清洗

数据清洗，主要使用到的框架就是pandas框架，功能很简单的分为两种：

1. 仅对数据进行简单的去重【以下所有的分析结果皆来自这个数据集（因为量大，高质量数据量太少了）】

2. 对爬取的数据不仅做了去重，同时对所有的用户进行筛选，如果用户数据中有任意一项是空着的，那么这个数据就会被删除，也就是说经过这个模式清洗后，所有的用户都是个人信息完备的【人类高质量数据（并不）】

#### 数据分析

**一元表&&二元表**

​	这里首先解释一下一元表和二元表的意思：

| 一元表     | 用户信息中的单属性人数统计，如：各个地区的人数分部，各个等级人数分部，各个行业的人数分部... |
| ---------- | ------------------------------------------------------------ |
| **二元表** | **用户信息中两个属性结合的人数统计，如：各个地区不同等级的人数分部，各个地区不同行业的人数分部，各个等级不同回答数量的人数分部....** |

​	上述俩表的共同点：人数是默认属性。有了上述的分析，不难得出，如果属性颠倒的话，大概有168个不同的分析表。所以本项目分析后的结果集是这样的：

![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/5.png)

![](https://raw.githubusercontent.com/srx-2000/git_spider/master/zhihu_user_info_spider/6.png)

**排行榜**

排行榜分为了以下6个榜单：

1. **水逼榜单**：知乎回答很多但获赞很少
2. **全站热榜**：知乎回答数量良好，同时平均回答水平很高
3. **新秀榜单**：知乎回答数量少，等级低，但回答平均水平很高
4. **超高质量榜**：知乎回答数量少，但回答水平极高，平均每个回答都有1w左右的点赞
5. **超核心用户**：每日在知乎问题热榜中留名的用户（按天排序）【主要是因为爬虫的传播方式】
6. **热搜榜**：主要依托于用户在我们网站中被检索的频次

排行榜上的算法如下（分为4个榜单）：

1. 筛选条件

   1. **水逼榜**：
      1. 等级≥8级
      2. 回答数量≥5000
      3. 每个回答平均点赞数量在【50,500】之间
      4. 被点赞总数≥250000
      5. 粉丝数量≥0
   2. **全站热榜**
      1. 等级≥7级
      2. 回答数量≥100
      3. 每个回答平均点赞数量在【500,5000】之间
      4. 被点赞总数≥500000
      5. 粉丝数量≥50000
   3. **新秀榜单**
      1. 等级≤5级
      2. 回答数量≤100
      3. 每个回答平均点赞数量在【400,5000】之间
      4. 被点赞总数≥400
      5. 粉丝数量≤1000
   4. **超高质量榜**
      1. 等级≥0级
      2. 回答数量≤100
      3. 每个回答平均点赞数量在【5000，无上限】之间
      4. 被点赞总数≥10000
      5. 粉丝数量≥0

2. 计算公式

   > 粉丝数：follower_count
   >
   > 产品数（回答数量+专栏数量+视频数量+文章数量）：product_count
   >
   > 被点赞总数：voteup_count
   >
   > 质量数（被点赞总数/回答数量）：quality

   1. **水逼榜**：

      `0.7*{answer_count}+0.3*{quality}`

   2. **全站热榜：**

      `0.1*{follower_count}+0.35*{product_count}+0.35*{voteup_count}+0.2*{quality}`

   3. **新秀榜单：**

      `0.3*{voteup_count}+0.2*{answer_count}+0.5*{quality}`

   4. **超高质量榜：**

      `{quality}`

#### 数据可视化

因为我们项目是有前端的，所以数据可视化的部分就放在了前端，使用echarts实现的，图的种类更多，也更帅，所以爬虫这里就没有做可视化。

#### 使用方法（仅提供了pycharm等编辑器的运行方式）

- **配置文件（如果没有特殊需求可以跳过这步）**

  1. 打开`zhihu_user_info_spider/data_analysis/util/config.json`文件配置其中以下属性：

     ```json
     "province":{}
     "single": [],
     "binary": [],
     "data_type_path": {},
     "algorithm_scope": {},
     "algorithm_formula": {}
     ```

     | province              | 中国市级映射表：基本不用动，这个是用来统计用户区域分布的，千万别删399、400这两行 |
     | --------------------- | ------------------------------------------------------------ |
     | **single**            | **一元表的属性，默认属性有：性别、等级、地区、行业、回答数量、问题数量、关注数量、粉丝数量、关注问题数量、关注话题数量、关注专栏数量、关注收藏夹数量，被点赞总数** |
     | **binary**            | **二元表的属性：默认属性为一元表中的所有属性进行组合而得【可能会有重合如：level-gender，gender-level。这两个属性的组合只是颠倒了数据其实是一样的，但出表之后的直观效果不同】** |
     | **data_type_path**    | **分析结果路径：默认为list，binary，single分别用来存储排行榜，二元表，一元表。如要新的表需要在这里添加路径** |
     | **algorithm_scope**   | **算法筛选范围：默认为上述6个排行榜中前四个的筛选范围**      |
     | **algorithm_formula** | **算法公式：默认为上述6个排行榜中前四个的公式**              |

- **数据清洗**

  1. 打开`zhihu_user_info_spider/data_analysis/cleanser/pandasCleanser.py`，点击运行，可以在**爬虫的result文件夹**中找到当日的保存路径，分析数据保存在其中。

- **数据分析**

  1. **二元表获取**：打开`zhihu_user_info_spider/data_analysis/analysis/BinaryPivotTable.py`，点击运行，可以在**数据分析的result文件夹**中找到当日的保存路径，分析数据保存在其中。

  2. **一元表获取**：打开`zhihu_user_info_spider/data_analysis/analysis/OneTuplePivotTable.py`，点击运行，可以在**数据分析的result文件夹**中找到当日的保存路径，分析数据保存在其中。
  3. **一元表获取**：打开`zhihu_user_info_spider/data_analysis/analysis/RankingList.py`，点击运行，可以在**数据分析的result文件夹**中找到当日的保存路径，分析数据保存在其中。

