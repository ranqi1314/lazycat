### xvideos视频爬虫

​	曾经有一份爬取pornhub的爬虫摆在我面前，但我没有珍惜，等到pornhub大清洗过后我才追悔莫及，尘世间最痛苦的事莫过于此。如果上天可以给我再来一次的机会，我一定全爬下来放到数据库中。

​	本来想爬pornhub的，但是pornhub这不经历了一次灾难后，把好多视频都删了而且现存的视频好多都需要付费下载。所以这次将矛头对准了另一家free的[网站](https://www.xvideos.com)。

​	**功能**

​	通过用户给出的关键词，爬取该关键词搜索到的所有的视频，并将这些视频以.mp4的的形式保存在本地电脑上（未来打算接入mongodb数据库，存在数据库中，以免未来再发生pornhub事件）

​	**环境**

1. windows10
2. python3.7

​	**使用方法**

1. 首先需要一个可以带动整个电脑科学上网的vpn或机场**这个大家自己找一下哈**
2. 需要使用到的库已经放在requirements.txt，使用pip安装的可以使用指令`pip install -r requirements.txt`。如果国内安装第三方库比较慢，可以使用以下指令进行清华源加速`pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/`
3. 如果出现了scrapy库安装失败的问题可以尝试这个命令进行安装`pip install scrapy -i http://pypi.douban.com/simple --trusted-host pypi.douban.com`
4. 具备了以上两个条件之后，我们就可以进入`search_parser.py`文件中将`key`修改成你要搜索的内容即可，然后运行`search_parser.py`即可，在运行之后应该会在video目录下得到一个`video_url.txt`文件，里面就是搜索到的所有的与关键词相关的页面了。
5. 之后，我们只需要直接进入`downloader.py`文件，运行即可。此时会在控制台让你分别输入，邮箱和密码（没有xvideos网站账号的去网站注册一个就OK，免费的）。输入邮箱和密码后，首先会解析刚刚获取到的所有的页面，而后会逐个下载，下载的mp4文件会存在video目录下。

> **注意** 在spiders目录下会有一个`cookies.txt`文件，其中保存着你登录后留下的cookies，是下载视频的必须品，所以不要轻易删掉该文件。【暂时已弃用】

​	**技术栈**

1. requests
2. json
3. parsel（xpath筛选器）
4. os
5. cookiejar（模拟登陆保存cookies）【暂时删去】
6. scrapy（自动迭代）~~上次自己写了一个递归迭代感觉太麻烦了，还是直接用现成的香~~
7. time
8. contextlib
9. queue
10. threading

​	**进步（相较于之前）**

1.  实现了模拟登陆（暂时由于Google的recaptcha验证而被迫将登陆功能取消）
2. 完成了scrapy框架和别的库的交叉使用
3. 实现了视频的保存（之前B站视频时通过第三方库下载的）

#### 更新日志

- 2021.1.8 

  第一次上传，更新了基本内容，未实现多线程下载以及接入mongodb。

- 2021.1.11

  实现了多线程下载，多线程对页面进行解析，同时优化了scrapy框架在获取相关页面的方式，大幅提升下载速度、页面解析速度，以及关键字搜索速度。

  同时出现了一个致命性的错误——xvideos在登录上使用了Google的recaptcha验证，而这个情况只有在Google的检测检测阈值到达一定程度后才会触发。所以对于刚使用的同学依旧可以使用代码中的模拟登陆实现，自动获取cookie的操作（只需要把`downloader.py`中的151~154行的注释去掉即可使用）。但是一旦被检测出来后，就需要使用者自己手动登陆，并使用抓包工具获取到cookie值放到`downloader.py`中的16行后面即可，同时一定要把自动登录那几行代码注释掉，要不会读取到无法使用的cookie。

  除此之外还有一个小问题就是xvideos有一定的下载数量限制，每个用户每天只能下载一定数量的视频，如果超了就会获取不到下载链接，所以还需要多准备几个账号才行。未来如果能找到解决recaptcha验证的方法，我会加入账号自动切换的功能的。

  仍未实现接入mongodb。
  
- 2021.8.15

  **优化**

  1. 修复了解析变更的问题【videos页面进行了更新导致之前版本的页面解析出现了问题，现已修复】。

   	2. 解决了解析与下载之间的矛盾问题

  **新问题**

  1. 由于现在xvideos已经开始使用recaptcha验证，而Google家的recaptcha又是著名的难搞，所以这里选择性的放弃了原来的登录功能，看看未来能不能在xvideos中找到一些漏洞绕开recaptcha，到之后再添加登录功能。
  2. 目前仍未接入mongodb主要的原因是：如果使用了mongodb那么就只能保存视频的连接地址了，这违背了我做这个爬虫的初衷，如果网站将视频下架了，那么空有连接地址依旧是下不了的，所以近期并不打算加入这个模块。

  > 由于本爬虫是直接将视频下载到本地存储的，而不是像其他爬虫那样保存视频下载地址，所以爬取的速度完全取决于用户自己电脑上的网速以及VPN的程度。

  **未来打算**

  1. 未来有可能会加入一个“继续下载”的功能，因为现在这个爬虫存在一个问题，就是如果用户在下载的时候如果中途退出，那么程序在启动后依旧会从第一个视频开始下载，这就会导致视频重复下载的问题，所以正在考虑解决这个问题。
  
- 2021.8.16

  **优化**

  1. 因为看到issue中有小伙伴使用的是cmd的运行方式，所以这个版本添加了cmd的运行方式，具体使用方式

     > 1. 安装好上面的所有库
     > 2. cd到..\hubSpider\hubSpider\spiders路径下
     > 3. 先运行python search_parser.py命令
     > 4. 等待运行结束后再运行python downloader.py命令即可开始爬取
     > 5. 视频下载成功后会存在..\hubSpider\hubSpider\video文件夹下
  
  **小建议**
  
  ​	由于我在编写这个爬虫时使用的是pycharm编辑器，所以这里还是推荐使用类似的ide运行爬虫，不论是操作上还是下载信息回馈的体验上都要远远优于命令行运行。

