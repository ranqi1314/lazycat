### b站视频爬虫

**功能**
爬取某个up的大部分视频

**使用到的库和框架**
scrapy+[you-get](https://github.com/soimort/you-get)

**使用方法**

1. 首先我们需要准备两个库，scrapy和you-get，只需要在命令行中输入以下两个命令即可

   > scrapy: `pip install scrapy -i http://pypi.douban.com/simple --trusted-host pypi.douban.com`
   >
   > you-get: `pip3 install you-get`

2. 然后我们需要在b站找到一个你想要爬取视频的up主，并进入到的个人空间获取url，如图所示：这里我们可以看见中间有一串数字，这就是这个up主的uid了，而我们的爬虫需要这个uid才可以运行。

   ![image-20201225210757643](C:\Users\16016\AppData\Roaming\Typora\typora-user-images\image-20201225210757643.png)
   
3. 我们除了需要这个uid，还需要一个该up主的任意一个视频，作为爬虫的起始地址，而后我们的爬虫会根据这个起始地址下的推荐进行检索，找到并跳转到这个up的其他的视频中，并不断递归下去，直到推荐中没有出现该up的其他视频为止，所以这里其实有个bug，就是无法爬取这个up的所有视频.......只能说是大部分。

4. 我们需要进入项目目录中的bilibili.py文件中将刚刚得到的uid写入到`up_uid`后面，再把那个任意视频的视频链接写到`base_url`后面即可。因为我是在win10的环境下开发的，所以这个爬虫爬下来的视频会直接放到D盘下的sp文件夹下。如果想要修改保存地址，只需要进入到SP_download.py文件下找到download函数，将`-o`后的路径改为你想要保存的路径即可。记得要在路径前加空格，别和`-o`连一起。

5. 在运行的时候，我们需要分两步运行，首先是需要在命令行中，cd到bilibiliSP这个目录下，然后运行`scrapy crawl bilibili`，如果没有报错的话，经过一顿输出后，应该会在该目录下生成一个bilibili_Output.txt文件，然后我们再运行SP_download.py文件即可直接将视频保存在相应的目录下。

**bug（以下的bug有可能会被修复）**

1. 首先就是无法获取一个up主下的所有视频，因为他是根据推荐视频进行检索的，所以如果推荐的视频里没有该up的视频，就无法进行递归了。
2. 由于you-get在下载b站视频时比较慢，所以我这里使用了python中的多线程进行下载，但并没有规定具体的线程数量，也没有使用线程池进行约束，所以具体能跑多少线程完全看电脑的性能，而且电脑在运行时应该也会比较卡，我个人电脑的话，好像可以同时开25个线程同时下载....也可能是我的错觉......
3. you-get这个下载库在结合多线程之后会出现一些小bug，导致有很多时候视频虽然获取到了，但是并没有下载成功，会报错，这个还得继续研究一下原因。
4. 开始了就没有退路可言！没错，再开始运行这个爬虫的第二阶段，也就是运行SP_download之后，程序是无法停止的。就我个人的经验而谈，我是使用pycharm运行的，所以要真想停止的话可以直接使用任务管理器将pycharm终止掉。具体在终端运行这个py文件时如何停止，那就不得而知了。

**程序改动**

因为我这个爬虫在视频检索时是使用推荐检索的所以，理论上是可以爬取B站所有同类型视频的（如果B站没有相关检测机制的话），只需要对我这个爬虫的源代码进行一定的删减就可以做到了（没错就是删减，因为对特定up的爬取反而多了一些检测代码）。

**在此特别感谢you-get的贡献者团队！**

**更新**

* 2021.6.24

  由于b站的html变动原因，上个版本的爬虫在递归获取url地址时出现了问题，现已修复。

